{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver as wd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.by import By \n",
    "import pandas as pd\n",
    "import time\n",
    "import urllib\n",
    "\n",
    "def get_article_info(driver, crawl_date, press_list, title_list, link_list, date_list, more_news_base_url=None, more_news=False):\n",
    "    more_news_url_list = []\n",
    "    while True:    \n",
    "        page_html_source = driver.page_source\n",
    "        url_soup = BeautifulSoup(page_html_source, 'lxml')\n",
    "        \n",
    "        more_news_infos = url_soup.select('a.news_more')\n",
    "        \n",
    "        if more_news:\n",
    "            for more_news_info in more_news_infos:\n",
    "                more_news_url = f\"{more_news_base_url}{more_news_info.get('href')}\"\n",
    "\n",
    "                more_news_url_list.append(more_news_url)\n",
    "\n",
    "        article_infos = url_soup.select(\"div.news_area\")\n",
    "        \n",
    "        if not article_infos:\n",
    "            break\n",
    "\n",
    "        for article_info in article_infos:  \n",
    "            press_info = article_info.select_one(\"div.info_group > a.info.press\")\n",
    "            \n",
    "            if press_info is None:\n",
    "                press_info = article_info.select_one(\"div.info_group > span.info.press\")\n",
    "            article = article_info.select_one(\"a.news_tit\")\n",
    "            \n",
    "            press = press_info.text.replace(\"언론사 선정\", \"\")\n",
    "            title = article.get('title')\n",
    "            link = article.get('href')\n",
    "\n",
    "#             print(f\"press - {press} / title - {title} / link - {link}\")\n",
    "            press_list.append(press)\n",
    "            title_list.append(title)\n",
    "            link_list.append(link)\n",
    "            date_list.append(crawl_date)\n",
    "\n",
    "        time.sleep(2.0)\n",
    "                      \n",
    "                      \n",
    "        next_button_status = url_soup.select_one(\"a.btn_next\").get(\"aria-disabled\")\n",
    "        \n",
    "        if next_button_status == 'true':\n",
    "            break\n",
    "        \n",
    "        time.sleep(1.0)\n",
    "        next_page_btn = driver.find_element(By.CSS_SELECTOR,\"a.btn_next\").click()      \n",
    "    \n",
    "    return press_list, title_list, link_list, more_news_url_list\n",
    "    \n",
    "    \n",
    "\n",
    "def get_naver_news_info_from_selenium(keyword, save_path, target_date, ds_de, sort=0, remove_duplicate=False):\n",
    "    crawl_date = f\"{target_date[:4]}.{target_date[4:6]}.{target_date[6:]}\"\n",
    "    driver = wd.Chrome(\"../chromedriver_win32 (1)/chromedriver.exe\") # chromedriver 파일 경로\n",
    "\n",
    "    encoded_keyword = urllib.parse.quote(keyword)\n",
    "    url = f\"https://search.naver.com/search.naver?where=news&query={encoded_keyword}&sm=tab_opt&sort={sort}&photo=0&field=0&pd=3&ds={ds_de}&de={ds_de}&docid=&related=0&mynews=0&office_type=0&office_section_code=0&news_office_checked=&nso=so%3Ar%2Cp%3Afrom{target_date}to{target_date}&is_sug_officeid=0\"\n",
    "    \n",
    "    more_news_base_url = \"https://search.naver.com/search.naver\"\n",
    "\n",
    "    driver.get(url)\n",
    "    \n",
    "    press_list, title_list, link_list, date_list, more_news_url_list = [], [], [], [], []\n",
    "    \n",
    "    press_list, title_list, link_list, more_news_url_list = get_article_info(driver=driver, \n",
    "                                                                             crawl_date=crawl_date, \n",
    "                                                                             press_list=press_list, \n",
    "                                                                             title_list=title_list, \n",
    "                                                                             link_list=link_list,\n",
    "                                                                             date_list=date_list,\n",
    "                                                                             more_news_base_url=more_news_base_url,\n",
    "                                                                             more_news=True)\n",
    "    driver.close()\n",
    "    \n",
    "    if len(more_news_url_list) > 0:\n",
    "        print(len(more_news_url_list))\n",
    "        more_news_url_list = list(set(more_news_url_list))\n",
    "        print(f\"->{len(more_news_url_list)}\")\n",
    "        for more_news_url in more_news_url_list:\n",
    "            driver = wd.Chrome(\"../chromedriver_win32 (1)/chromedriver.exe\")\n",
    "            driver.get(more_news_url)\n",
    "            \n",
    "            press_list, title_list, link_list, more_news_url_list = get_article_info(driver=driver, \n",
    "                                                                             crawl_date=crawl_date, \n",
    "                                                                             press_list=press_list, \n",
    "                                                                             title_list=title_list, \n",
    "                                                                             link_list=link_list,\n",
    "                                                                             date_list=date_list)\n",
    "            driver.close()\n",
    "    article_df = pd.DataFrame({\"날짜\": date_list, \"언론사\": press_list, \"제목\": title_list, \"링크\": link_list})\n",
    "    \n",
    "    print(f\"extract article num : {len(article_df)}\")\n",
    "    if remove_duplicate:\n",
    "        article_df = article_df.drop_duplicates(['링크'], keep='first')\n",
    "        print(f\"after remove duplicate -> {len(article_df)}\")\n",
    "    \n",
    "    article_df.to_excel(save_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "def crawl_news_data(keyword, year, month, start_day, end_day, save_path):\n",
    "    for day in tqdm(range(start_day, end_day+1)):\n",
    "        date_time_obj = datetime(year=year, month=month, day=day)\n",
    "        target_date = date_time_obj.strftime(\"%Y%m%d\")\n",
    "        ds_de = date_time_obj.strftime(\"%Y.%m.%d\")\n",
    "\n",
    "        get_naver_news_info_from_selenium(keyword=keyword, save_path=f\"{save_path}/{keyword}/{target_date}_{keyword}_.xlsx\", target_date=target_date, ds_de=ds_de, remove_duplicate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "keywords = ['빈지노']\n",
    "save_path = 'D:/big16/final-dev'\n",
    "\n",
    "for keyword in keywords:\n",
    "    os.makedirs(f\"{save_path}/{keyword}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from selenium import webdriver\n",
    "# import time\n",
    "\n",
    "# url = 'https://n.news.naver.com/article/023/0003770350?ntype=RANKING'\n",
    "\n",
    "# driver = webdriver.Chrome(executable_path='./chromedriver.exe')\n",
    "# driver.implicitly_wait(5)\n",
    "# driver.get(url)\n",
    "\n",
    "# while True:\n",
    "#     try:\n",
    "#         더보기 = driver.find_element_by_css_selector('a.u_cbox_btn_more')\n",
    "#         더보기.click()\n",
    "#         time.sleep(1)\n",
    "#     except:\n",
    "#         break\n",
    "\n",
    "# print(\"end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start keyword - 빈지노 crawling ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_6048\\3847549332.py:61: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = wd.Chrome(\"../chromedriver_win32 (1)/chromedriver.exe\") # chromedriver 파일 경로\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extract article num : 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:05<00:00,  5.92s/it]\n"
     ]
    }
   ],
   "source": [
    "for keyword in keywords:\n",
    "    print(f\"start keyword - {keyword} crawling ...\")\n",
    "    crawl_news_data(keyword=keyword, year=2023, month=1, start_day=1, end_day=1, save_path=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "final-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
